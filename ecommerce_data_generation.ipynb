{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1WcLbZJ8mREXpptg45PX1uBKi6Mnnkp-O",
      "authorship_tag": "ABX9TyM52/QyKmzyQw+zdetkU9WW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fahad-Blog/Data-Science-Portfolio/blob/main/ecommerce_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7JO2qvERU67",
        "outputId": "781491f6-986a-43ba-ae28-5df8f09a88c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting faker\n",
            "  Downloading faker-38.2.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Downloading faker-38.2.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-38.2.0\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy faker"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Below step creates dummified Dimension tables\n"
      ],
      "metadata": {
        "id": "ZWx2LGVFRk5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "from datetime import date, timedelta\n",
        "import random\n",
        "\n",
        "fake = Faker()\n",
        "\n",
        "# --- 1. Dim_Date (Required for all facts) ---\n",
        "def generate_dim_date(start_date, end_date):\n",
        "    date_list = pd.date_range(start_date, end_date).tolist()\n",
        "    data = []\n",
        "    for d in date_list:\n",
        "        data.append({\n",
        "            'date_key': int(d.strftime('%Y%m%d')),\n",
        "            'full_date': d.date(),\n",
        "            'year': d.year,\n",
        "            'month': d.month,\n",
        "            'day_of_week': d.day_of_week,\n",
        "            'is_weekend': d.day_of_week >= 5 # 5=Sat, 6=Sun\n",
        "        })\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# --- 2. Dim_Product (50 SKUs) ---\n",
        "def generate_dim_product(num_products=50):\n",
        "    categories = ['Apparel', 'Accessories', 'Home Goods', 'Electronics']\n",
        "    data = []\n",
        "    for i in range(1, num_products + 1):\n",
        "        sku = f\"SKU-{1000 + i}\"\n",
        "        category = np.random.choice(categories, p=[0.4, 0.3, 0.2, 0.1])\n",
        "        data.append({\n",
        "            'product_key': i,\n",
        "            'product_id': fake.uuid4(),\n",
        "            'product_name': f\"{category} Product {i}\",\n",
        "            'sku': sku,\n",
        "            'category': category,\n",
        "            'unit_cost': round(random.uniform(5.0, 50.0), 2)\n",
        "        })\n",
        "    df = pd.DataFrame(data)\n",
        "    # Simulate a data quality issue: missing cost for a few products\n",
        "    df.loc[df.sample(frac=0.04).index, 'unit_cost'] = np.nan\n",
        "    return df\n",
        "\n",
        "# --- 3. Dim_Customer (200 Customers) ---\n",
        "def generate_dim_customer(num_customers=200):\n",
        "    segments = ['New', 'Returning', 'VIP', 'Lapsed']\n",
        "    data = []\n",
        "    for i in range(1, num_customers + 1):\n",
        "        data.append({\n",
        "            'customer_key': i,\n",
        "            'customer_id': fake.uuid4(),\n",
        "            'first_name': fake.first_name(),\n",
        "            'last_name': fake.last_name(),\n",
        "            'email': fake.email(),\n",
        "            'city': fake.city(),\n",
        "            'country': 'USA', # Keep simple for now\n",
        "            'segment': np.random.choice(segments, p=[0.3, 0.4, 0.2, 0.1])\n",
        "        })\n",
        "    df = pd.DataFrame(data)\n",
        "    # Simulate data governance issue: duplicate customer records\n",
        "    duplicate_rows = df.iloc[[5, 10, 15]].copy()\n",
        "    duplicate_rows['customer_key'] = np.arange(201, 204) # Assign new keys\n",
        "    df = pd.concat([df, duplicate_rows], ignore_index=True)\n",
        "    return df.reset_index(drop=True)\n",
        "\n",
        "# --- 4. Dim_Channel (Marketing Channels) ---\n",
        "def generate_dim_channel():\n",
        "    data = [\n",
        "        {'channel_key': 1, 'channel_source': 'Google', 'channel_medium': 'cpc', 'campaign_name': 'Brand_Search'},\n",
        "        {'channel_key': 2, 'channel_source': 'Facebook', 'channel_medium': 'cpc', 'campaign_name': 'Retargeting_Ads'},\n",
        "        {'channel_key': 3, 'channel_source': 'Google', 'channel_medium': 'organic', 'campaign_name': 'SEO'},\n",
        "        {'channel_key': 4, 'channel_source': 'Email', 'channel_medium': 'newsletter', 'campaign_name': 'Weekly_Promo'},\n",
        "        {'channel_key': 5, 'channel_source': 'Direct', 'channel_medium': 'none', 'campaign_name': 'Direct_Entry'},\n",
        "        {'channel_key': 6, 'channel_source': 'Affiliate', 'channel_medium': 'referral', 'campaign_name': 'Top_Bloggers'}\n",
        "    ]\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "# --- Run Dimension Generation ---\n",
        "start_date = date(2025, 10, 1)\n",
        "end_date = date(2025, 12, 7)\n",
        "\n",
        "df_dim_date = generate_dim_date(start_date, end_date)\n",
        "df_dim_product = generate_dim_product()\n",
        "df_dim_customer = generate_dim_customer()\n",
        "df_dim_channel = generate_dim_channel()\n",
        "\n",
        "print(f\"Generated {len(df_dim_date)} dates, {len(df_dim_product)} products, {len(df_dim_customer)} customers.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJJgGIVrRbW2",
        "outputId": "2625a20d-7a52-46d9-f4f7-484580fcdef3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 68 dates, 50 products, 203 customers.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries (ensure pandas, numpy, faker are installed)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from faker import Faker\n",
        "import random\n",
        "\n",
        "# Reinitialize Faker\n",
        "fake = Faker()\n",
        "\n",
        "# --- 5. Fact_Order & Fact_Order_Item (3,000 Orders) ---\n",
        "def generate_fact_orders(df_dim_date, df_dim_customer, df_dim_channel, df_dim_product, num_orders=3000):\n",
        "\n",
        "    # --- 5a. Setup Valid Foreign Keys ---\n",
        "    valid_dates = df_dim_date['date_key'].tolist()\n",
        "    valid_customers = df_dim_customer['customer_key'].tolist()\n",
        "    valid_channels = df_dim_channel['channel_key'].tolist()\n",
        "\n",
        "    # 6 Channel Keys require 6 probabilities that sum to 1.0:\n",
        "    channel_probabilities = [0.2, 0.15, 0.1, 0.1, 0.35, 0.1]\n",
        "\n",
        "    # 5b. Fact_Order generation\n",
        "    order_data = []\n",
        "\n",
        "    for i in range(1, num_orders + 1):\n",
        "        date_key = random.choice(valid_dates)\n",
        "        customer_key = random.choice(valid_customers)\n",
        "\n",
        "        # CORRECTED np.random.choice: Array size matches probability list size\n",
        "        channel_key = np.random.choice(\n",
        "            valid_channels,\n",
        "            p=channel_probabilities\n",
        "        )\n",
        "\n",
        "        order_data.append({\n",
        "            'order_key': i,\n",
        "            'order_id': fake.bothify(text='ORD-########'),\n",
        "            'date_key': date_key,\n",
        "            'customer_key': customer_key,\n",
        "            'channel_key': channel_key,\n",
        "            'total_order_amount': 0.0, # Will be updated later\n",
        "            'tax_amount': 0.0, # Will be updated later\n",
        "            'shipping_cost': np.random.choice([0.0, 5.0, 10.0], p=[0.7, 0.2, 0.1]),\n",
        "            'discount_amount': round(random.uniform(0.0, 50.0), 2) if random.random() < 0.2 else 0.0\n",
        "        })\n",
        "\n",
        "    df_fact_order = pd.DataFrame(order_data)\n",
        "\n",
        "    # 5c. Fact_Order_Item generation\n",
        "    item_data = []\n",
        "    order_item_key = 1\n",
        "\n",
        "    valid_products = df_dim_product['product_key'].tolist()\n",
        "    # Create a dictionary for quick lookup of product costs (handles potential NaNs)\n",
        "    product_costs = df_dim_product.set_index('product_key')['unit_cost'].to_dict()\n",
        "\n",
        "    for index, order in df_fact_order.iterrows():\n",
        "        num_items = random.randint(1, 3)\n",
        "        order_total_gross = 0.0\n",
        "\n",
        "        for _ in range(num_items):\n",
        "            product_key = random.choice(valid_products)\n",
        "            quantity = random.randint(1, 4)\n",
        "\n",
        "            # Retrieve cost, defaulting to 25.0 if product_costs has a NaN value (The data quality issue)\n",
        "            unit_price = product_costs.get(product_key)\n",
        "            if pd.isna(unit_price):\n",
        "                # Placeholder for null price handling\n",
        "                unit_price = 25.0\n",
        "\n",
        "            item_revenue = round(quantity * unit_price, 2)\n",
        "            order_total_gross += item_revenue\n",
        "\n",
        "            item_data.append({\n",
        "                'order_item_key': order_item_key,\n",
        "                'order_key': order['order_key'],\n",
        "                'product_key': product_key,\n",
        "                'quantity': quantity,\n",
        "                'unit_price': unit_price,\n",
        "                'gross_revenue': item_revenue\n",
        "            })\n",
        "            order_item_key += 1\n",
        "\n",
        "        # Update Fact_Order total amount\n",
        "        total_after_discount = order_total_gross - order['discount_amount']\n",
        "        tax = round(total_after_discount * 0.08, 2)\n",
        "        final_total = total_after_discount + tax + order['shipping_cost']\n",
        "\n",
        "        df_fact_order.loc[index, 'total_order_amount'] = round(final_total, 2)\n",
        "        df_fact_order.loc[index, 'tax_amount'] = tax\n",
        "\n",
        "    df_fact_order_item = pd.DataFrame(item_data)\n",
        "\n",
        "    return df_fact_order, df_fact_order_item\n",
        "\n",
        "# --- 6. Fact_Inventory (Daily snapshot per product) ---\n",
        "def generate_fact_inventory(df_dim_date, df_dim_product):\n",
        "    inventory_data = []\n",
        "    inventory_key = 1\n",
        "\n",
        "    for date_key, date_row in df_dim_date.iterrows():\n",
        "        for product_key in df_dim_product['product_key'].tolist():\n",
        "            initial_stock = 100\n",
        "\n",
        "            # Simple stock fluctuation simulation\n",
        "            quantity_on_hand = max(0, initial_stock + random.randint(-20, 10))\n",
        "\n",
        "            inventory_data.append({\n",
        "                'inventory_key': inventory_key,\n",
        "                'date_key': date_row['date_key'],\n",
        "                'product_key': product_key,\n",
        "                'quantity_on_hand': quantity_on_hand,\n",
        "                'quantity_in_transit': random.randint(0, 50),\n",
        "                'reorder_point': 50\n",
        "            })\n",
        "            inventory_key += 1\n",
        "\n",
        "    return pd.DataFrame(inventory_data)\n",
        "\n",
        "# --- 7. Fact_Traffic (Simulated Google Analytics hits) ---\n",
        "def generate_fact_traffic(df_dim_date, df_dim_customer, df_dim_channel, num_sessions=50000):\n",
        "    traffic_data = []\n",
        "\n",
        "    valid_dates = df_dim_date['date_key'].tolist()\n",
        "    valid_channels = df_dim_channel['channel_key'].tolist()\n",
        "\n",
        "    # --- CORRECTED CUSTOMER SELECTION LOGIC ---\n",
        "\n",
        "    # 1. Define the possible choices for customer_key: all valid keys + a placeholder for NaN\n",
        "    valid_customers = df_dim_customer['customer_key'].tolist()\n",
        "    N = len(valid_customers)\n",
        "\n",
        "    # Choices: [1, 2, 3, ..., 200, 'UNIDENTIFIED']\n",
        "    choices = valid_customers + ['UNIDENTIFIED']\n",
        "\n",
        "    # Probabilities: 90% chance of being identified (spread across N customers), 10% chance of being unidentified\n",
        "    P_identified = 0.9 / N\n",
        "    probabilities = [P_identified] * N + [0.1]\n",
        "\n",
        "    # Sanity Check: Ensure probabilities sum to 1.0\n",
        "    if not 0.999 < sum(probabilities) < 1.001:\n",
        "         raise ValueError(\"Traffic probabilities do not sum to 1.0\")\n",
        "\n",
        "    for i in range(1, num_sessions + 1):\n",
        "        channel_key = random.choice(valid_channels)\n",
        "        date_key = random.choice(valid_dates)\n",
        "\n",
        "        # Select the choice using the correctly sized arrays\n",
        "        selected_choice = np.random.choice(choices, p=probabilities)\n",
        "\n",
        "        # Map the placeholder back to NaN/invalid key\n",
        "        if selected_choice == 'UNIDENTIFIED':\n",
        "            customer_key = np.nan\n",
        "        else:\n",
        "            customer_key = selected_choice\n",
        "\n",
        "        traffic_data.append({\n",
        "            'traffic_key': i,\n",
        "            'date_key': date_key,\n",
        "            'customer_key': customer_key, # This contains NaNs, ready for cleaning!\n",
        "            'channel_key': channel_key,\n",
        "            'session_id': fake.uuid4(),\n",
        "            'page_views': random.randint(1, 10),\n",
        "            'session_duration_seconds': random.randint(5, 300),\n",
        "            'bounce_rate': round(random.random(), 2)\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(traffic_data)\n",
        "    # Simulate a key ETL issue: some customer keys are explicitly set to 0/invalid (not NaN)\n",
        "    df.loc[df.sample(frac=0.01).index, 'customer_key'] = 0\n",
        "    return df\n",
        "\n",
        "# --- Execution Block (Place at the end of your complete script) ---\n",
        "# NOTE: Replace with your actual dimension DataFrames if running as a standalone block!\n",
        "# Example placeholder execution:\n",
        "df_fact_order, df_fact_order_item = generate_fact_orders(df_dim_date, df_dim_customer, df_dim_channel, df_dim_product)\n",
        "df_fact_inventory = generate_fact_inventory(df_dim_date, df_dim_product)\n",
        "df_fact_traffic = generate_fact_traffic(df_dim_date, df_dim_customer, df_dim_channel)\n",
        "\n",
        "print(f\"Generated {len(df_fact_order)} orders, {len(df_fact_order_item)} order items, {len(df_fact_inventory)} inventory records, and {len(df_fact_traffic)} traffic sessions.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xchjOftoR7Ee",
        "outputId": "63d4be58-f299-44c4-fd17-171390681f6f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated 3000 orders, 6067 order items, 3400 inventory records, and 50000 traffic sessions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the Generated File into CSV"
      ],
      "metadata": {
        "id": "5XN19JXWVmqj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DEBUGGING VERSION ---\n",
        "output_dir = 'C:/ecommerce_data'\n",
        "import os\n",
        "\n",
        "print(f\"Attempting to create directory: {output_dir}\")\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "print(\"Directory creation check passed.\")\n",
        "\n",
        "# --- Export DataFrames to CSV ---\n",
        "df_dim_date.to_csv(f'{output_dir}dim_date.csv', index=False)\n",
        "df_dim_product.to_csv(f'{output_dir}dim_product.csv', index=False)\n",
        "df_dim_customer.to_csv(f'{output_dir}dim_customer.csv', index=False)\n",
        "df_dim_channel.to_csv(f'{output_dir}dim_channel_n.csv', index=False)\n",
        "\n",
        "df_fact_order.to_csv(f'{output_dir}fact_order.csv', index=False)\n",
        "df_fact_order_item.to_csv(f'{output_dir}fact_order_item.csv', index=False)\n",
        "df_fact_inventory.to_csv(f'{output_dir}fact_inventory.csv', index=False)\n",
        "df_fact_traffic.to_csv(f'{output_dir}fact_traffic.csv', index=False)\n",
        "\n",
        "print(f\"\\nAll raw data files exported successfully to: {output_dir}\")\n",
        "\n",
        "print(f\"\\nSUCCESS! Check this exact path for files: {output_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckuDLqSVVrh0",
        "outputId": "2aaca3c3-2e4c-47b7-8b4c-e638063e3ae1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create directory: C:/ecommerce_data\n",
            "Directory creation check passed.\n",
            "\n",
            "All raw data files exported successfully to: C:/ecommerce_data\n",
            "\n",
            "SUCCESS! Check this exact path for files: C:/ecommerce_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PrMZ-rpubSQ1"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0e63fb1"
      },
      "source": [
        "# Task\n",
        "Upload the `df_dim_channel` DataFrame to a BigQuery table named `dim_channel`. Specify your Google Cloud `project_id` and the `dataset_id` where the table should be created."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dd193ed9"
      },
      "source": [
        "## Authenticate to Google Cloud\n",
        "\n",
        "### Subtask:\n",
        "Authenticate your Google Cloud account to allow access to BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d1ebea6e"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires authentication to Google Cloud, which can be done by importing the `google.colab.auth` module and calling its `authenticate_user()` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77a888bb"
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nB5hD-7Jm7p"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8c376531"
      },
      "source": [
        "# Task\n",
        "Install the `google-cloud-bigquery` library and then upload the `df_dim_channel` DataFrame to a BigQuery table named `dim_channel` within your specified \"project_id\" and \"dataset_id\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cac54556"
      },
      "source": [
        "## Install BigQuery Library\n",
        "\n",
        "### Subtask:\n",
        "Install the `google-cloud-bigquery` library to enable interaction with Google BigQuery.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "238171f8"
      },
      "source": [
        "**Reasoning**:\n",
        "To interact with Google BigQuery, the `google-cloud-bigquery` library needs to be installed using pip.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76f7683a",
        "outputId": "4432933b-9a69-46cd-ec1c-8beb33cc4998"
      },
      "source": [
        "get_ipython().system('pip install google-cloud-bigquery')"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google-cloud-bigquery in /usr/local/lib/python3.12/dist-packages (3.38.0)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=2.11.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (2.28.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.43.0)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.5.0)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.8.0)\n",
            "Requirement already satisfied: packaging>=24.2.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (25.0)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.9.0.post0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery) (2.32.4)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.72.0)\n",
            "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.19.5 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (6.33.2)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-api-core<3.0.0,>=2.11.1->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.26.1)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (1.76.0)\n",
            "Requirement already satisfied: cachetools<7.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (6.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (4.9.1)\n",
            "Requirement already satisfied: google-crc32c<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from google-resumable-media<3.0.0,>=2.0.0->google-cloud-bigquery) (1.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3.0.0,>=2.8.2->google-cloud-bigquery) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.21.0->google-cloud-bigquery) (2025.11.12)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio<2.0.0,>=1.33.2->google-api-core[grpc]<3.0.0,>=2.11.1->google-cloud-bigquery) (4.15.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-bigquery) (0.6.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.cloud import bigquery\n",
        "import pandas as pd\n",
        "import pandas_gbq\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "project_id = 'ecommerce-data-480511'       # Your Google Cloud Project ID\n",
        "dataset_id = 'ecommerce_data_17021995'     # Your BigQuery Dataset ID\n",
        "# IMPORTANT: Update this path to the exact directory where you saved the CSVs\n",
        "local_csv_path = '/content/C:'\n",
        "# NOTE: Ensure you have the necessary authentication set up (e.g., gcloud auth application-default login)\n",
        "\n",
        "# --- 2. LIST OF TABLES TO UPLOAD ---\n",
        "# This list contains the base name of the CSV file and the target BigQuery table name\n",
        "csv_to_table_map = [\n",
        "    'ecommerce_datadim_date',\n",
        "    'ecommerce_datadim_product',\n",
        "    'ecommerce_datadim_customer',\n",
        "    'ecommerce_datadim_channel_n',\n",
        "    'ecommerce_datafact_order',\n",
        "    'ecommerce_datafact_order_item',\n",
        "    'ecommerce_datafact_inventory',\n",
        "    'ecommerce_datafact_traffic',\n",
        "]\n",
        "\n",
        "# Initialize a BigQuery client\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# --- 3. UPLOAD PROCESS ---\n",
        "print(f\"üöÄ Starting bulk upload to BigQuery Dataset: {dataset_id}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for table_name in csv_to_table_map:\n",
        "    csv_file = f\"{table_name}.csv\"\n",
        "    full_csv_path = os.path.join(local_csv_path, csv_file)\n",
        "    full_table_id = f\"{project_id}.{dataset_id}.{table_name}\"\n",
        "\n",
        "    try:\n",
        "        # Step A: Read the local CSV file into a pandas DataFrame\n",
        "        # We assume the columns generated are consistent with the data types\n",
        "        df_to_upload = pd.read_csv(full_csv_path)\n",
        "\n",
        "        # Step B: Upload the DataFrame to BigQuery\n",
        "        # if_exists='replace' ensures a clean load for the raw data layer\n",
        "        pandas_gbq.to_gbq(\n",
        "            df_to_upload,\n",
        "            full_table_id,\n",
        "            project_id=project_id,\n",
        "            if_exists='replace',\n",
        "            progress_bar=True\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ Successfully uploaded {csv_file} ({len(df_to_upload):,} rows) to: {full_table_id}\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"‚ùå Error: CSV file not found at {full_csv_path}. Skipping.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to upload {csv_file}. Error: {e}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(\"‚úÖ Bulk upload process complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPpIwzqqO3hb",
        "outputId": "db976fd1-058f-476e-949b-bcc9b55370b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting bulk upload to BigQuery Dataset: ecommerce_data_17021995\n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 10155.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datadim_date.csv (68 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datadim_date\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 9686.61it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datadim_product.csv (50 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datadim_product\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 11397.57it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datadim_customer.csv (203 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datadim_customer\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13573.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datadim_channel_n.csv (6 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datadim_channel_n\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 8289.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datafact_order.csv (3,000 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datafact_order\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 12336.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datafact_order_item.csv (6,067 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datafact_order_item\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13486.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datafact_inventory.csv (3,400 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datafact_inventory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 13273.11it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Successfully uploaded ecommerce_datafact_traffic.csv (50,000 rows) to: ecommerce-data-480511.ecommerce_data_17021995.ecommerce_datafact_traffic\n",
            "--------------------------------------------------\n",
            "‚úÖ Bulk upload process complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "PROJECT_ID = \"ecommerce-data-480511\"\n",
        "PROFILE_NAME = \"ecommerce_portfolio\"\n",
        "PROJECT_DIR = \"my_portfolio\"\n",
        "\n",
        "\n",
        "!mkdir -p ~/.dbt\n",
        "\n",
        "profiles_content = f\"\"\"\n",
        "{PROFILE_NAME}:\n",
        "  target: dev\n",
        "  outputs:\n",
        "    dev:\n",
        "      type: bigquery\n",
        "      method: oauth\n",
        "      project: {PROJECT_ID}\n",
        "      dataset: analytics_portfolio\n",
        "      threads: 4\n",
        "      timeout_seconds: 300\n",
        "      location: US\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.expanduser('~/.dbt/profiles.yml'), 'w') as f:\n",
        "    f.write(profiles_content)\n",
        "print(\"‚úÖ profiles.yml recreated successfully.\")\n",
        "\n",
        "\n",
        "os.makedirs(f\"{PROJECT_DIR}/models\", exist_ok=True)\n",
        "\n",
        "dbt_project_content = f\"\"\"\n",
        "name: 'my_portfolio'\n",
        "version: '1.0.0'\n",
        "config-version: 2\n",
        "\n",
        "# This MUST match the profile name defined above\n",
        "profile: '{PROFILE_NAME}'\n",
        "\n",
        "model-paths: [\"models\"]\n",
        "analysis-paths: [\"analyses\"]\n",
        "test-paths: [\"tests\"]\n",
        "seed-paths: [\"seeds\"]\n",
        "macro-paths: [\"macros\"]\n",
        "snapshot-paths: [\"snapshots\"]\n",
        "\n",
        "target-path: \"target\"\n",
        "clean-targets:\n",
        "  - \"target\"\n",
        "  - \"dbt_packages\"\n",
        "\n",
        "models:\n",
        "  my_portfolio:\n",
        "    staging:\n",
        "      +materialized: view\n",
        "    marts:\n",
        "      +materialized: table\n",
        "\"\"\"\n",
        "\n",
        "with open(f\"{PROJECT_DIR}/dbt_project.yml\", 'w') as f:\n",
        "    f.write(dbt_project_content)\n",
        "print(\"‚úÖ dbt_project.yml recreated successfully.\")\n",
        "\n",
        "\n",
        "print(\"\\nüîÑ Running dbt debug...\")\n",
        "\n",
        "%cd {PROJECT_DIR}\n",
        "!dbt debug"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtO0N4GCDQsC",
        "outputId": "3577bf28-a0a4-4223-d501-7e7e9739438c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ profiles.yml recreated successfully.\n",
            "‚úÖ dbt_project.yml recreated successfully.\n",
            "\n",
            "üîÑ Running dbt debug...\n",
            "/content/my_portfolio/my_portfolio\n",
            "\u001b[0m17:43:12  Running with dbt=1.11.0-rc2\n",
            "\u001b[0m17:43:12  dbt version: 1.11.0-rc2\n",
            "\u001b[0m17:43:12  python version: 3.12.12\n",
            "\u001b[0m17:43:12  python path: /usr/bin/python3\n",
            "\u001b[0m17:43:12  os info: Linux-6.6.105+-x86_64-with-glibc2.35\n",
            "\u001b[0m17:43:18  Using profiles dir at /root/.dbt\n",
            "\u001b[0m17:43:18  Using profiles.yml file at /root/.dbt/profiles.yml\n",
            "\u001b[0m17:43:18  Using dbt_project.yml file at /content/my_portfolio/my_portfolio/dbt_project.yml\n",
            "\u001b[0m17:43:18  adapter type: bigquery\n",
            "\u001b[0m17:43:18  adapter version: 1.10.3\n",
            "\u001b[0m17:43:19  Configuration:\n",
            "\u001b[0m17:43:19    profiles.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
            "\u001b[0m17:43:19    dbt_project.yml file [\u001b[32mOK found and valid\u001b[0m]\n",
            "\u001b[0m17:43:19  Required dependencies:\n",
            "\u001b[0m17:43:19   - git [\u001b[32mOK found\u001b[0m]\n",
            "\n",
            "\u001b[0m17:43:19  Connection:\n",
            "\u001b[0m17:43:19    method: oauth\n",
            "\u001b[0m17:43:19    database: ecommerce-data-480511\n",
            "\u001b[0m17:43:19    execution_project: ecommerce-data-480511\n",
            "\u001b[0m17:43:19    schema: analytics_portfolio\n",
            "\u001b[0m17:43:19    location: US\n",
            "\u001b[0m17:43:19    priority: None\n",
            "\u001b[0m17:43:19    maximum_bytes_billed: None\n",
            "\u001b[0m17:43:19    impersonate_service_account: None\n",
            "\u001b[0m17:43:19    job_retry_deadline_seconds: None\n",
            "\u001b[0m17:43:19    job_retries: 1\n",
            "\u001b[0m17:43:19    job_creation_timeout_seconds: None\n",
            "\u001b[0m17:43:19    job_execution_timeout_seconds: 300\n",
            "\u001b[0m17:43:19    timeout_seconds: 300\n",
            "\u001b[0m17:43:19    client_id: None\n",
            "\u001b[0m17:43:19    token_uri: None\n",
            "\u001b[0m17:43:19    compute_region: None\n",
            "\u001b[0m17:43:19    dataproc_cluster_name: None\n",
            "\u001b[0m17:43:19    gcs_bucket: None\n",
            "\u001b[0m17:43:19    dataproc_batch: None\n",
            "\u001b[0m17:43:19  Registered adapter: bigquery=1.10.3\n",
            "\u001b[0m17:43:20    Connection test: [\u001b[32mOK connection ok\u001b[0m]\n",
            "\n",
            "\u001b[0m17:43:20  \u001b[32mAll checks passed!\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}