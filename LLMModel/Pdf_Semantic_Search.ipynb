{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOltSlLNVBr7tepf0sQAddA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fahad-Blog/Data-Science-Portfolio/blob/main/Pdf_Semantic_Search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pymongo import MongoClient\n",
        "from pypdf import PdfReader  # NEW: Library to read PDFs\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "MONGO_URI = \"Instruction : Get your MongoDB URI by creating a new cluster\"\n",
        "DB_NAME = \"feedback_db\"\n",
        "COLLECTION_NAME = \"resume_pdf_file\"\n",
        "PDF_PATH = \"/Bio.pdf\" # NEW: Path to your PDF\n",
        "\n",
        "# 1. Connect to MongoDB\n",
        "try:\n",
        "    client = MongoClient(MONGO_URI)\n",
        "    db = client[DB_NAME]\n",
        "    collection = db[COLLECTION_NAME]\n",
        "    print(\"‚úÖ Connected to MongoDB Atlas\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Connection failed: {e}\")\n",
        "    exit()\n",
        "\n",
        "# 2. Load the Embedding Model\n",
        "print(\"‚è≥ Loading AI Model (this happens once)...\")\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# --- NEW SECTION: PDF PROCESSING & CHUNKING ---\n",
        "\n",
        "def extract_and_chunk_pdf(file_path, chunk_size=500, overlap=50):\n",
        "    \"\"\"\n",
        "    Reads a PDF and splits it into smaller text chunks.\n",
        "    chunk_size: Number of characters per chunk.\n",
        "    overlap: Number of characters to repeat between chunks (prevents context loss).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        reader = PdfReader(file_path)\n",
        "        full_text = \"\"\n",
        "\n",
        "        # Extract text from all pages\n",
        "        for page in reader.pages:\n",
        "            text = page.extract_text()\n",
        "            if text:\n",
        "                full_text += text + \"\\n\"\n",
        "\n",
        "        # Sliding Window Chunking Logic\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        text_length = len(full_text)\n",
        "\n",
        "        while start < text_length:\n",
        "            # Define the end of the chunk\n",
        "            end = start + chunk_size\n",
        "\n",
        "            # Create the chunk\n",
        "            chunk = full_text[start:end]\n",
        "\n",
        "            # Clean up newlines for better embedding quality\n",
        "            clean_chunk = chunk.replace('\\n', ' ').strip()\n",
        "\n",
        "            if len(clean_chunk) > 10: # Filter out tiny empty chunks\n",
        "                chunks.append(clean_chunk)\n",
        "\n",
        "            # Move the window forward, minus the overlap\n",
        "            start += (chunk_size - overlap)\n",
        "\n",
        "        print(f\"üìÑ Processed PDF. Extracted {len(chunks)} chunks.\")\n",
        "        return chunks\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error reading PDF: {e}\")\n",
        "        return []\n",
        "\n",
        "# 3. Prepare Data (PDF instead of CSV)\n",
        "# We convert the chunks into the dictionary format the rest of the script expects\n",
        "print(\"üìÇ Reading and chunking PDF...\")\n",
        "pdf_chunks = extract_and_chunk_pdf(PDF_PATH)\n",
        "\n",
        "# Convert list of strings to list of dicts (to match previous dataframe structure)\n",
        "documents_to_insert = []\n",
        "for chunk in pdf_chunks:\n",
        "    documents_to_insert.append({\n",
        "        \"text\": chunk,\n",
        "        \"source\": \"Bio.pdf\",  # Metadata to know where it came from\n",
        "        \"type\": \"pdf_fragment\"\n",
        "    })\n",
        "\n",
        "# 4. Generate Embeddings & Insert Data\n",
        "print(\"üöÄ Generating Embeddings and Indexing Data...\")\n",
        "\n",
        "# Process the prepared documents\n",
        "final_docs = []\n",
        "for doc in documents_to_insert:\n",
        "    # Text -> Vector\n",
        "    vector_embedding = model.encode(doc['text']).tolist()\n",
        "\n",
        "    # Add embedding to the document\n",
        "    doc['embedding'] = vector_embedding\n",
        "    final_docs.append(doc)\n",
        "\n",
        "# Insert into MongoDB\n",
        "if len(final_docs) > 0:\n",
        "    # Optional: Clear old data if you want a fresh start\n",
        "    # collection.delete_many({})\n",
        "\n",
        "    collection.insert_many(final_docs)\n",
        "    print(f\"‚úÖ {len(final_docs)} chunked documents inserted into MongoDB!\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No data found to insert.\")\n",
        "\n",
        "# --- THE SEARCH PHASE ---\n",
        "\n",
        "def semantic_search(query, limit=2):\n",
        "    print(f\"\\nüîé Searching for: '{query}'\")\n",
        "\n",
        "    query_vector = model.encode(query).tolist()\n",
        "\n",
        "    pipeline = [\n",
        "        {\n",
        "            \"$vectorSearch\": {\n",
        "                \"index\": \"vector_index_pdf\",\n",
        "                \"path\": \"embedding\",\n",
        "                \"queryVector\": query_vector,\n",
        "                \"numCandidates\": 100,\n",
        "                \"limit\": limit\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"$project\": {\n",
        "                \"_id\": 0,\n",
        "                \"text\": 1,\n",
        "                \"score\": {\"$meta\": \"vectorSearchScore\"}\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    results = collection.aggregate(pipeline)\n",
        "\n",
        "    for result in results:\n",
        "        # Print only the first 200 chars of the result to keep output clean\n",
        "        preview = result['text'][:200] + \"...\"\n",
        "        print(f\" [Score: {result['score']:.4f}] {preview}\")\n",
        "\n",
        "# Test Cases (Adjust these based on the content of Bio.pdf)\n",
        "semantic_search(\"Glass\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkapBn2Bwqkv",
        "outputId": "c22e1945-5873-45a4-b968-c34b915dc80b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Connected to MongoDB Atlas\n",
            "‚è≥ Loading AI Model (this happens once)...\n",
            "üìÇ Reading and chunking PDF...\n",
            "üìÑ Processed PDF. Extracted 22 chunks.\n",
            "üöÄ Generating Embeddings and Indexing Data...\n",
            "‚úÖ 22 chunked documents inserted into MongoDB!\n",
            "\n",
            "üîé Searching for: 'Glass'\n",
            " [Score: 0.6477] nd corrected critical quality parameters using advanced Statistical Process Control (SPC) tools  and design  experiments, ensuring high product reliability and process compliance.      Quality Assuran...\n",
            " [Score: 0.6477] nd corrected critical quality parameters using advanced Statistical Process Control (SPC) tools  and design  experiments, ensuring high product reliability and process compliance.      Quality Assuran...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dk-nA2m257W7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
